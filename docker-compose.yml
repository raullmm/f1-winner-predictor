version: "3.9"

services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    container_name: mlflow
    ports:
      - "${MLFLOW_PORT:-5001}:5001"
    command: >
      mlflow server
      --host 0.0.0.0 --port 5001
      --backend-store-uri sqlite:///mlflow.db
      --default-artifact-root /mlruns
    volumes:
      - mlruns:/mlruns
    env_file: .env

  train:
    build:
      context: .
      dockerfile: api/Dockerfile.train
    env_file: .env
    volumes:
      - mlruns:/mlruns    
      - data-cache:/app/data
    depends_on:
      - mlflow
    ports:
      - "7000:7000"

  predict:
    build:
      context: .
      dockerfile: api/Dockerfile.predict
    container_name: f1-predict
    environment:
      - MLFLOW_URI=http://mlflow:5001
      - MODEL_STAGE=Production
    volumes:
      - data-cache:/app/data
      - mlruns:/mlruns
    ports:
      - "8080:8000"
    depends_on:
      train:
        condition: service_completed_successfully
      mlflow:
        condition: service_started

volumes:
  mlruns:
  data-cache:
